{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@authors\n",
    "* Arseniy Ashuha, you can text me ```ars.ashuha@gmail.com```,\n",
    "* Based on https://github.com/ebenolson/pydata2015\n",
    "\n",
    "<h1 align=\"center\"> Part II: Attention mechanism @ Image Captioning </h1> \n",
    "\n",
    "<img src=\"https://s2.postimg.org/pq18f5t7t/deepbb.png\" width=480>\n",
    "\n",
    "In this seminar you'll be going through the image captioning pipeline.\n",
    "\n",
    "To begin with, let us download the dataset of image features from a pre-trained GoogleNet (see instructions in chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import numpy as np\n",
    "\n",
    "captions = np.load(\"./data/train-data-captions.npy\")\n",
    "img_codes = np.load(\"./data/train-data-svdfeatures.npy\").astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each image code is a 6x6 feature matrix from GoogleNet: (82783, 128, 6, 6)\n",
      "[-19.53911972   3.23637891   2.08816719   0.66493636  -2.9185071\n",
      "   1.82758021  -1.3254329   -0.53197509  -3.15473676  -1.8739953 ]\n",
      "\n",
      "\n",
      "\n",
      "for each image there are 5-7 descriptions, e.g.:\n",
      "\n",
      "People shopping in an open market for vegetables.\n",
      "An open market full of people and piles of vegetables.\n",
      "People are shopping at an open air produce market.\n",
      "Large piles of carrots and potatoes at a crowded outdoor market.\n",
      "People shop for vegetables like carrots and potatoes at an open air market.\n"
     ]
    }
   ],
   "source": [
    "print (\"each image code is a 6x6 feature matrix from GoogleNet:\", img_codes.shape)\n",
    "print (img_codes[0,:10,0,0])\n",
    "print ('\\n\\n')\n",
    "print (\"for each image there are 5-7 descriptions, e.g.:\\n\")\n",
    "print ('\\n'.join(captions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split descriptions into tokens\n",
    "for img_i in range(len(captions)):\n",
    "    for caption_i in range(len(captions[img_i])):\n",
    "        sentence = captions[img_i][caption_i] \n",
    "        captions[img_i][caption_i] = [\"#START#\"]+sentence.split(' ')+[\"#END#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Vocabulary\n",
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "for img_captions in captions:\n",
    "    for caption in img_captions:\n",
    "        word_counts.update(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab  = ['#UNK#', '#START#', '#END#']\n",
    "vocab += [k for k, v in word_counts.items() if v >= 5]\n",
    "vocab = list(set(vocab))\n",
    "n_tokens = len(vocab)\n",
    "\n",
    "assert 12000 <= n_tokens <= 15000\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this function to convert sentences into a network-readible matrix of token indices.\n",
    "\n",
    "When given several sentences of different length, it pads them with -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_ix = -1\n",
    "UNK_ix = vocab.index('#UNK#')\n",
    "START_ix = vocab.index(\"#START#\")\n",
    "END_ix = vocab.index(\"#END#\")\n",
    "\n",
    "#good old as_matrix for the third time\n",
    "def as_matrix(sequences,max_len=None):\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int32')+PAD_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word,UNK_ix) for word in seq[:max_len]]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def to_string(tokens_ix):\n",
    "    assert len(np.shape(tokens_ix))==1,\"to_string works on one sequence at a time\"\n",
    "    tokens_ix = list(tokens_ix)[1:]\n",
    "    if END_ix in tokens_ix:\n",
    "        tokens_ix = tokens_ix[:tokens_ix.index(END_ix)]\n",
    "    return \" \".join([vocab[i] for i in tokens_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try it out on several descriptions of a random image\n",
    "as_matrix(captions[1337])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_string(as_matrix(captions[1337])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network\n",
    "\n",
    "Since the image encoder CNN is already applied, the only remaining part is to write a sentence decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano, theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "# network shapes. \n",
    "EMBEDDING_SIZE = 128    #Change at your will\n",
    "LSTM_SIZE  = 256        #Change at your will\n",
    "ATTN_SIZE  = 256        #Change at your will\n",
    "FEATURES,HEIGHT,WIDTH = img_codes.shape[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a single LSTM step here. An LSTM step should\n",
    "* take previous cell/out and input\n",
    "* compute next cell/out and next token probabilities\n",
    "* use attention to work with image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<Your attention layers>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.resolver import ProbabilisticResolver\n",
    "from agentnet.memory import LSTMCell\n",
    "\n",
    "temperature = theano.shared(1.)\n",
    "class decoder:\n",
    "    prev_word = InputLayer((None,),name='index of previous word')\n",
    "    image_features = InputLayer((None,FEATURES,HEIGHT,WIDTH),name='img features')\n",
    "\n",
    "    prev_cell = InputLayer((None,LSTM_SIZE),name='previous LSTM cell goes here')\n",
    "    prev_out = InputLayer((None,LSTM_SIZE),name='previous LSTM output goes here')\n",
    "    \n",
    "    prev_word_emb = EmbeddingLayer(prev_word,len(vocab),EMBEDDING_SIZE)\n",
    "    \n",
    "    ###Attention part:\n",
    "    # Please implement attention part of rnn architecture\n",
    "    \n",
    "    #First we reshape image into a sequence of image vectors\n",
    "    image_features_seq = reshape(dimshuffle(image_features,[0,2,3,1]),[[0],-1,[3]])\n",
    "    \n",
    "    #Then we apply attention just as usual\n",
    "    attn_probs = <Compute attention probabilities>\n",
    "    attn = <Compute attention result given probabilities>\n",
    "\n",
    "    lstm_input = concat([attn,prev_word_emb],axis=-1)\n",
    "\n",
    "    new_cell,new_out = LSTMCell(prev_cell,prev_out,lstm_input)\n",
    "    \n",
    "    \n",
    "    output_probs = DenseLayer(new_out,len(vocab),nonlinearity=T.nnet.softmax)\n",
    "\n",
    "    \n",
    "    output_probs_scaled = ExpressionLayer(output_probs,lambda p: p**temperature)\n",
    "    output_tokens = ProbabilisticResolver(output_probs_scaled,assume_normalized=False)\n",
    "    \n",
    "    \n",
    "    # recurrent state transition dict\n",
    "    # on next step, {key} becomes {value}\n",
    "    transition = {\n",
    "        new_cell:prev_cell,\n",
    "        new_out:prev_out\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "During training, we should feed our decoder RNN with reference captions from the dataset. Training then comes down to simple likelihood maximization problem.\n",
    "\n",
    "Deep learning people also know this as minimizing crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for sentences\n",
    "sentences = T.imatrix(\"[batch_size x time] of word ids\")\n",
    "l_sentences = InputLayer((None,None),sentences)\n",
    "\n",
    "# Input layer for image features\n",
    "image_vectors = T.tensor4(\"image features [batch,channels,h,w]\")\n",
    "l_image_features = InputLayer((None,FEATURES,HEIGHT,WIDTH),image_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet import Recurrence\n",
    "\n",
    "decoder_trainer = Recurrence(\n",
    "    input_sequences={decoder.prev_word:l_sentences},\n",
    "    input_nonsequences={decoder.image_features:l_image_features},\n",
    "    state_variables=decoder.transition,\n",
    "    tracked_outputs=[decoder.output_probs],\n",
    "    unroll_scan = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions and define loss\n",
    "next_token_probs = get_output(decoder_trainer[decoder.output_probs])\n",
    "\n",
    "next_token_probs = next_token_probs[:,:-1].reshape([-1,len(vocab)])\n",
    "next_tokens = sentences[:,1:].ravel()\n",
    "\n",
    "loss = T.nnet.categorical_crossentropy(next_token_probs,next_tokens)\n",
    "\n",
    "#apply mask\n",
    "mask = T.neq(next_tokens,PAD_ix)\n",
    "loss = T.sum(loss*mask)/T.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainable NN weights\n",
    "weights = get_all_params(decoder_trainer,trainable=True)\n",
    "updates = lasagne.updates.adam(loss,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile a functions for training and evaluation\n",
    "#please not that your functions must accept image features as FIRST param and sentences as second one\n",
    "train_step = theano.function([image_vectors,sentences],loss,updates=updates,allow_input_downcast=True)\n",
    "val_step   = theano.function([image_vectors,sentences],loss,allow_input_downcast=True)\n",
    "#for val_step use deterministic=True if you have any dropout/noize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "* You first have to implement a batch generator\n",
    "* Than the network will get trained the usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def generate_batch(images,captions,batch_size,max_caption_len=None):\n",
    "    \n",
    "    #sample random numbers for image/caption indicies\n",
    "    random_image_ix = np.random.randint(0,len(images),size=batch_size)\n",
    "    \n",
    "    #get images\n",
    "    batch_images = images[random_image_ix]\n",
    "    \n",
    "    #5-7 captions for each image\n",
    "    captions_for_batch_images = captions[random_image_ix]\n",
    "    \n",
    "    #pick 1 from 5-7 captions for each image\n",
    "    batch_captions = list(map(choice,captions_for_batch_images))\n",
    "    \n",
    "    #convert to matrix\n",
    "    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n",
    "    \n",
    "    return batch_images, batch_captions_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx,by = generate_batch(img_codes,captions,3)\n",
    "bx[0,:10,0,0],by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "* We recommend you to periodically evaluate the network using the next \"apply trained model\" block\n",
    " *  its safe to interrupt training, run a few examples and start training again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=50 #adjust me\n",
    "n_epochs=100 #adjust me\n",
    "n_batches_per_epoch = 50 #adjust me\n",
    "n_validation_batches = 5 #how many batches are used for validation after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss=0\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        train_loss += train_step(*generate_batch(img_codes,captions,batch_size))\n",
    "    train_loss /= n_batches_per_epoch\n",
    "    \n",
    "    \n",
    "    print('Epoch: {}, train loss: {}'.format(epoch, train_loss))\n",
    "\n",
    "print(\"Finish :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = theano.shared(np.int32(1))\n",
    "MAX_LENGTH = 20         #Change at your will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up recurrent network that generates tokens and feeds them back to itself\n",
    "unroll_dict = dict(decoder.transition)\n",
    "unroll_dict[decoder.output_tokens] = decoder.prev_word #on next iter, output goes to input\n",
    "\n",
    "first_output = T.repeat(T.constant(START_ix,dtype='int32'),batch_size)\n",
    "init_dict = {\n",
    "    decoder.output_tokens:InputLayer([None],first_output)\n",
    "}\n",
    "\n",
    "decoder_applier = Recurrence(\n",
    "    input_nonsequences={decoder.image_features:l_image_features},\n",
    "    state_variables=unroll_dict,\n",
    "    state_init = init_dict,\n",
    "    tracked_outputs=[decoder.output_probs,decoder.output_tokens],\n",
    "    n_steps = MAX_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = get_output(decoder_applier[decoder.output_tokens])\n",
    "\n",
    "generate = theano.function([image_vectors],generated_tokens,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrained_lenet import image_to_features\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = plt.imread(\"./data/Dog-and-Cat.jpg\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ix = generate([image_to_features(img)])[0]\n",
    "\n",
    "for _ in range(100):\n",
    "    temperature.set_value(10)\n",
    "    print to_string(output_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tricks (for further research)\n",
    "\n",
    "* Initialize LSTM with some function of image features.\n",
    "\n",
    "* Try other attention functions\n",
    "\n",
    "* If you train large network, it is usually a good idea to make a 2-stage prediction\n",
    "    1. (large recurrent state) -> (bottleneck e.g. 256)\n",
    "    2. (bottleneck) -> (vocabulary size)\n",
    "    * this way you won't need to store/train (large_recurrent_state x vocabulary size) matrix\n",
    "    \n",
    "* Use [hierarchical softmax](https://gist.github.com/justheuristic/581853c6d6b87eae9669297c2fb1052d) or [byte pair encodings](https://github.com/rsennrich/subword-nmt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
