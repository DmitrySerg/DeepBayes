{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "* Alexandr Panin, Arseniy Ashuha, you can text me ```ars.ashuha@gmail.com```,\n",
    "* Based on https://github.com/ebenolson/pydata2015\n",
    "\n",
    "\n",
    "<h1 align=\"center\"> Part I: Attention mechanism at toy problems </h1> \n",
    "\n",
    "<img src=\"https://s2.postimg.org/pq18f5t7t/deepbb.png\" width=480>\n",
    "\n",
    "In this seminar you will implement attention mechanism and apply it to a simple task of associative recall.\n",
    "\n",
    "# Install me:\n",
    "```(bash)\n",
    "sudo pip install --upgrade https://github.com/yandexdataschool/agentnet/archive/master.zip\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(\"/Users/dmitrys/anaconda/lib/python3.6/site-packages\")\n",
    "import agentnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import numpy as np\n",
    "from lasagne.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import theano,theano.tensor as T\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description:\n",
    "\n",
    "You are given a sequence of pairs [key,value]. \n",
    "\n",
    "Both keys and values are one-hot encoded integers. \n",
    "\n",
    "The network should learn to generate values in order of ascension of keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_SIZE = 10\n",
    "def generate_sample(min_length = 3, max_length = 10, code_size=CODE_SIZE):\n",
    "    assert code_size >= max_length\n",
    "    length = np.random.randint(min_length, max_length)\n",
    "    \n",
    "    keys = np.random.permutation(length)\n",
    "    values = np.random.permutation(length)\n",
    "    input_pairs = zip(keys,values)\n",
    "    \n",
    "    input_1hot = np.zeros([length+1,code_size*2])\n",
    "    for i,(k,v) in enumerate(input_pairs):\n",
    "        input_1hot[i+1][k] = 1\n",
    "        input_1hot[i+1][code_size + v] = 1\n",
    "    \n",
    "    sorted_pairs = sorted(input_pairs,key=lambda(k,v):k)\n",
    "    \n",
    "    target_1hot = np.zeros([length+1,code_size*2])\n",
    "    for i,(k,v) in enumerate(sorted_pairs):\n",
    "        target_1hot[i+1][k] = 1\n",
    "        target_1hot[i+1][code_size + v] = 1\n",
    "    \n",
    "    \n",
    "    return input_1hot,target_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------KEY--------- +++++++++VAL+++++++++\n",
      "Input pairs:\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  0.]]\n",
      "Target pairs:\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "inp,out = generate_sample(max_length=5,code_size=5)\n",
    "print('-'*9 + \"KEY\" + '-'*9 + ' ' + '+'*9 + \"VAL\" + \"+\"*9)\n",
    "print(\"Input pairs:\\n\",inp)\n",
    "print(\"Target pairs:\\n\",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention!\n",
    "\n",
    "We're now going to implement attention mechanism, or more specifically, _additive attention_ (a.k.a. Bahdanau's attention).\n",
    "\n",
    "We'll do so in two steps:\n",
    "\n",
    "* __AttentionWeights(encoder_seq,attn_query)__ - a layer that returns attention weights (aka probabilities of taking each value).\n",
    "* __AttentionOutput(encoder_seq,attn_weights)__ - a layer that averages inputs given probabilities from AttentionWeights.\n",
    "\n",
    "If you're not feeling familiar with this procedure, just follow the step-by-step instructions in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagne.init import Normal\n",
    "class AttentionWeights(MergeLayer):\n",
    "    def __init__(self, encoder_seq, attn_query, num_units):\n",
    "        MergeLayer.__init__(self, [encoder_seq, attn_query])\n",
    "        \n",
    "        enc_units = encoder_seq.output_shape[2]\n",
    "        dec_units = attn_query.output_shape[1]\n",
    "        \n",
    "        self.W_enc = self.add_param(Normal(), (enc_units, num_units), name='enc_to_hid')\n",
    "        self.W_query = self.add_param(Normal(), (dec_units, num_units), name='dec_to_hid')\n",
    "        self.W_out = self.add_param(Normal(), (num_units, 1),name='hid_to_logit')\n",
    "    \n",
    "    def get_output_for(self, inputs):\n",
    "        # the encoder_sequence shape = [batch, time,units]\n",
    "        # the query shapeshape  = [batch, units]\n",
    "        encoder_sequence, query = inputs\n",
    "        \n",
    "        # Hidden layer activations, shape [batch,seq_len,hid_units]\n",
    "        \n",
    "        query_to_hid = query.dot(self.W_query)[:,None,:]\n",
    "        \n",
    "        enc_to_hid = self.W_query.dot(query) + self.W_enc.dot(encoder_sequence)\n",
    "        \n",
    "        hid = T.tanh(enc_to_hid.dot(self.W_out))\n",
    "        \n",
    "        # Logits from hidden, [batch_size, seq_len]\n",
    "        logits = self.W_out.dot(hid)\n",
    "        logits = logits[:,:,0]\n",
    "        \n",
    "        assert logits.ndim ==2, \"Logits must have shape [batch,time] and be 2-dimensional.\"\\\n",
    "                                \"Current amount of dimensions:\"+str(logits.ndim)\n",
    "        \n",
    "        attn_weights = T.nnet.softmax(logits)\n",
    "        \n",
    "        return attn_weights\n",
    "    \n",
    "    def get_output_shape_for(self,input_shapes):\n",
    "        enc_shape,query_shape = input_shapes\n",
    "        return enc_shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionOutput(MergeLayer):\n",
    "    def __init__(self, encoder_seq, attn_weights):\n",
    "        MergeLayer.__init__(self,[encoder_seq,attn_weights])\n",
    "    \n",
    "    def get_output_for(self,inputs):\n",
    "        # encoder_sequence shape = [batch,time,units]\n",
    "        # attn_weights shape = [batch,time]\n",
    "        encoder_sequence, attn_weights = inputs\n",
    "    \n",
    "        #Reshape attn_weights to make 'em 3-dimensional: [batch,time,1] - so you could multiply by encoder sequence\n",
    "        attn_weights = attn_weights.reshape([attn_weights.shape[0],attn_weights.shape[1],1])\n",
    "        \n",
    "        #Compute attention response by summing encoder elements with weights along time axis (axis=1)\n",
    "        attn_output = encoder_sequence.dot(attn_weights)\n",
    "        \n",
    "        return attn_output\n",
    "    \n",
    "    def get_output_shape_for(self,input_shapes):\n",
    "        enc_shape,query_shape = input_shapes\n",
    "        return (enc_shape[0],enc_shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a single step of recurrent neural network using attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.itensor3(\"Input tokens [batch,time,code]\")\n",
    "reference_answers = T.itensor3(\"Reference answers[batch,time,code]\")\n",
    "\n",
    "l_inputs = InputLayer((None,None,CODE_SIZE*2),input_sequence)\n",
    "l_prev_answers = InputLayer((None,None,CODE_SIZE*2),reference_answers[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.memory import RNNCell\n",
    "class step:\n",
    "    prev_output = InputLayer((None, CODE_SIZE*2), name='previous output')\n",
    "    input_sequence = InputLayer((None, None, CODE_SIZE*2), name='input sequence for attention')\n",
    "    prev_rnn = InputLayer((None, 64), name='last rnn state')\n",
    "    \n",
    "    #TODO your code here\n",
    "    attention_weights = AttentionWeights(input_sequence, prev_rnn,32)\n",
    "    attention_value = AttentionOutput(input_sequence, attention_weights)\n",
    "    \n",
    "    new_rnn = RNNCell(prev_rnn,concat([attention_value, prev_output]))\n",
    "    \n",
    "    output_probs = DenseLayer(\n",
    "        concat([new_rnn,attention_value]),\n",
    "        num_units=CODE_SIZE*2, nonlinearity=T.nnet.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet import Recurrence\n",
    "#This layer applies RNN to itself in a symbolic loop.\n",
    "#Please wait for DeepBayes' staff to explain how it works.\n",
    "\n",
    "rnn = Recurrence(\n",
    "    input_sequences    = {step.prev_output: l_prev_answers},\n",
    "    input_nonsequences = {step.input_sequence: l_inputs},\n",
    "    state_variables    = {step.new_rnn: step.prev_rnn},\n",
    "    tracked_outputs    = [step.output_probs,step.attention_weights],\n",
    "    unroll_scan=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Join() can only join tensors with the same number of dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-477d173d16ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m output_probs,attn_weights = get_output(\n\u001b[0;32m----> 2\u001b[0;31m     [rnn[step.output_probs], rnn[step.attention_weights]])\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m predict = theano.function(\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference_answers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/lasagne/layers/helper.pyc\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(layer_or_layers, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                                  \u001b[0;34m\"mapping this layer to an input expression.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                                  % layer)\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mall_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 accepted_kwargs |= set(utils.inspect_kwargs(\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/agentnet/agent/recurrence.pyc\u001b[0m in \u001b[0;36mget_output_for\u001b[0;34m(self, inputs, accumulate_updates, recurrence_flags, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m         input_feed_dict = dict(zip(list(chain(self.input_nonsequences.keys(), self.input_sequences.keys())),\n\u001b[1;32m    518\u001b[0m                                    list(chain(nonsequences,[seq[:,0] for seq in sequences]))))\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0minitial_output_fillers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrecurrence_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;31m# disable broadcasting of zeros_like(v) along all axes (since lasagne outputs are non-broadcastable)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         initial_output_fillers = [T.unbroadcast(T.zeros_like(v),*range(v.ndim))\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/agentnet/agent/recurrence.pyc\u001b[0m in \u001b[0;36mget_one_step\u001b[0;34m(self, prev_states, current_inputs, **get_output_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mlayer_or_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mget_output_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         )\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/lasagne/layers/helper.pyc\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(layer_or_layers, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                                  \u001b[0;34m\"mapping this layer to an input expression.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                                  % layer)\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mall_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 accepted_kwargs |= set(utils.inspect_kwargs(\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/lasagne/layers/merge.pyc\u001b[0m in \u001b[0;36mget_output_for\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautocrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcropping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatLayer\u001b[0m  \u001b[0;31m# shortcut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(tensor_list, axis)\u001b[0m\n\u001b[1;32m   4721\u001b[0m             \u001b[0;34m\"or a list, make sure you did not forget () or [] around \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4722\u001b[0m             \"arguments of concatenate.\", tensor_list)\n\u001b[0;32m-> 4723\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(axis, *tensors_list)\u001b[0m\n\u001b[1;32m   4470\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensors_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4471\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4472\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjoin_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \"\"\"\n\u001b[1;32m    614\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'return_list'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_test_value\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'off'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mmake_node\u001b[0;34m(self, *axis_and_tensors)\u001b[0m\n\u001b[1;32m   4203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4204\u001b[0m         return self._make_node_internal(\n\u001b[0;32m-> 4205\u001b[0;31m             axis, tensors, as_tensor_variable_args, output_maker)\n\u001b[0m\u001b[1;32m   4206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4207\u001b[0m     def _make_node_internal(self, axis, tensors,\n",
      "\u001b[0;32m/Users/dmitrys/anaconda/envs/python2/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36m_make_node_internal\u001b[0;34m(self, axis, tensors, as_tensor_variable_args, output_maker)\u001b[0m\n\u001b[1;32m   4269\u001b[0m         if not python_all([x.ndim == len(bcastable)\n\u001b[1;32m   4270\u001b[0m                            for x in as_tensor_variable_args[1:]]):\n\u001b[0;32m-> 4271\u001b[0;31m             raise TypeError(\"Join() can only join tensors with the same \"\n\u001b[0m\u001b[1;32m   4272\u001b[0m                             \"number of dimensions.\")\n\u001b[1;32m   4273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Join() can only join tensors with the same number of dimensions."
     ]
    }
   ],
   "source": [
    "output_probs,attn_weights = get_output(\n",
    "    [rnn[step.output_probs], rnn[step.attention_weights]])\n",
    "\n",
    "predict = theano.function(\n",
    "    [input_sequence,reference_answers],\n",
    "    [output_probs,attn_weights],\n",
    "    allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next_answers = reference_answers[:,1:]\n",
    "\n",
    "loss = -T.log(output_probs)*next_answers -T.log(1-output_probs)*(1-next_answers)\n",
    "loss = T.mean(loss)\n",
    "\n",
    "updates = lasagne.updates.adam(loss, get_all_params(rnn, trainable=True))\n",
    "\n",
    "train = theano.function([input_sequence, reference_answers], loss, updates=updates,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "from IPython.display import clear_output\n",
    "loss_history = []\n",
    "\n",
    "for i in tnrange(10000):\n",
    "    bx,by = generate_sample()\n",
    "    loss_history.append(train([bx],[by]))\n",
    "    \n",
    "    if i%500==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(loss_history)\n",
    "        plt.show()\n",
    "        \n",
    "        #draw attention map\n",
    "        bx,by = generate_sample()\n",
    "        probs,attentions = predict([bx],[by])\n",
    "\n",
    "        input_kv = zip(bx[:,:CODE_SIZE].argmax(-1),bx[:,CODE_SIZE:].argmax(-1))\n",
    "        target_kv = zip(by[:,:CODE_SIZE].argmax(-1),by[:,CODE_SIZE:].argmax(-1))\n",
    "        plt.imshow(attentions[0])\n",
    "        plt.xticks(*zip(*enumerate(map(str,input_kv))),rotation=45)\n",
    "        plt.yticks(*zip(*enumerate(map(str,target_kv))),rotation=45)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {
    "1efdd72be63d457dafc441ce841e39f5": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
