{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Обучение на частично размеченной выборке*\n",
    "\n",
    "Дополнительные материалы к семинару. По мотивам статьи [\"Semi-supervised Learning with\n",
    "Deep Generative Models\"](https://arxiv.org/pdf/1406.5298.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import load_dataset, iterate_minibatches\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Для этого задания мы повысим размерность скрытых компонент, а также случайным образом \"выбросим\" приблизительно 95% меток классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "HIDDEN_DIM = 16\n",
    "NUMBER_OF_DIGITS = 10\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "present = np.random.rand(X_train.shape[0]) < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Классы для распределений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BinaryVector():\n",
    "    def __init__(self, logits, rng=None):\n",
    "        self.rng = rng if rng else RandomStreams(lasagne.random.get_rng().randint(1,2147462579))\n",
    "        self.logits = logits\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        pixelwise_log_probs = (\n",
    "            x * (self.logits - T.nnet.softplus(self.logits))\n",
    "            - (1 - x) * T.nnet.softplus(self.logits)\n",
    "        )\n",
    "        return T.sum(pixelwise_log_probs, axis=(1, 2, 3))\n",
    "    \n",
    "    def sample(self):\n",
    "        shape = self.logits.shape\n",
    "        return T.nnet.sigmoid(self.logits) >= self.rng.uniform(shape)\n",
    "\n",
    "class MultivariateNormalDiag():\n",
    "    def __init__(self, loc=None, scale=None, rng=None):\n",
    "        self.rng = rng if rng else RandomStreams(lasagne.random.get_rng().randint(1,2147462579))\n",
    "        self.loc= loc\n",
    "        self.scale = scale\n",
    "    \n",
    "    def log_prob(self, z):\n",
    "        normalization_constant = (\n",
    "            - 0.5 * np.log(2 * np.pi)\n",
    "            - T.log(self.scale)\n",
    "        )\n",
    "        square_term = -0.5 * ((z - self.loc) / self.scale) ** 2\n",
    "        log_prob_vec = normalization_constant + square_term\n",
    "        return T.sum(log_prob_vec, axis=1)\n",
    "    \n",
    "    def sample(self):\n",
    "        shape = self.loc.shape\n",
    "        z = (self.loc + self.scale * self.rng.normal(shape))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Вероятностная модель данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В отличие от вариационного автокодировщика, генеративная модель теперь будет также включать и метки классов $y$:\n",
    "\n",
    "\\begin{align*}\n",
    "& p(x, y, z) = p(x | y, z) p(z) p(y) \\\\\n",
    "& p(y) = Cat(y | \\pi), \\pi = (1/10, \\dots, 1/10) \\\\\n",
    "& p(z) = \\mathcal N(z | 0, I) \\\\\n",
    "& p(x | y, z) = \\prod_{i=1}^D p_i(y, z)^{x_i} (1 - p_i(y, z))^{1 - x_i}\n",
    "\\end{align*}\n",
    "\n",
    "При обучении вариационного автокодировщика максимизируется маргинальное правдоподобие $\\log p(x)$ (нижняя оценка на него, если быть точным), а в данном случае мы будем максимизировать $\\log p(x,y)$ для объектов с метками и $\\log p(x)$ для объектов без метки. Обозначим за $P$ индексы объектов обучающей выборки с метками класса.\n",
    "\n",
    "Построим нижнюю оценку для\n",
    "\n",
    "\\begin{equation}\n",
    "L(X, y) = \\sum_{i \\notin P} \\log p(x_i) + \\sum_{i \\in P} \\log p(x_i, y_i)\n",
    "\\end{equation}\n",
    "\n",
    "Для этого определим следующее вариационное приближение:\n",
    "\n",
    "\\begin{align*}\n",
    "& q(y, z | x) = q(y | x) q(z | y, x)\\\\\n",
    "& \\\\\n",
    "& q(y | x) = Cat(y | \\pi(x))\\\\\n",
    "& q(z | y, x) = \\mathcal N(z | \\mu_\\phi(x, y), \\operatorname{diag}\\sigma^2(y, x))\n",
    "\\end{align*}\n",
    "\n",
    "### Оценка для $i \\in P$\n",
    "\n",
    "Случай похож на модель для вариационного автокодировщика\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(x, y) = \\log \\mathbb E_{p(z)} p(x, y | z) \\geq \\mathbb E_{q(z | y, x)} \\log \\frac{p(x, y|z) p(z)}{q(z | y, x)}\n",
    "\\end{equation}\n",
    "\n",
    "### Оценка $i \\notin P$\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(x) = \\log \\mathbb E_{p(y)} \\mathbb E_{p(z | y)} \\log p(x| z, y)\\geq \\mathbb E_{q(y | x)} \\mathbb E_{q(z | y, x)} \\log \\frac{p(x, y, z)}{q(z | y, x) q(y | x)}\n",
    "\\end{equation}\n",
    "\n",
    "### Целевая функия\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal L(X, y) = \\sum_{i \\in P} \\mathbb E_{q(z_i | y_i, x_i)} \\log \\frac{p(x_i, y_i, z_i)}{q(z_i | y_i, x_i)} + \\sum_{i \\notin P} \\mathbb E_{q(y_i | x_i)} \\mathbb E_{q(z_i | y_i, x_i)} \\log \\frac{p(x_i, y_i, z_i)}{q(z_i | y_i, x_i) q(y_i | x_i)}\n",
    "\\end{equation}\n",
    "\n",
    "Оценку для математического ожидания по $z$ будет получать с помощью *reparametrization trick*.\n",
    "Пользуясь малым количеством классов, математическое ожидание по $y$ будем вычислять явно.\n",
    "\n",
    "# Как заставить модель все-таки обучаться?\n",
    "\n",
    "Максимизация нижней оценки на обоснованность на практике может не приводит к построению хорошей модели вывода $q(y | x)$.\n",
    "\n",
    "Естественно искать модели $q(y | x)$ среди тех, которые согласуются с размеченными объектами обучающей выборки $(x_i, y_i)$. В статье, в которой была впервые предложена описанная в семинаре модель, с весом $\\alpha$ добавляется дополнительное слагаемое к функции потерь:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{|P|}\\sum_{i \\in P} y_i^T \\log q(y | x).\n",
    "\\end{equation}\n",
    "\n",
    "Оно соответствует кросс-энтропии классификатора $q(y|x)$ на размеченных объектах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Особенности реализации\n",
    "В данной реализации мы передаем на вход кодировщика и декодировщика one-hot коды для $y$.\n",
    "\n",
    "Это находит свое отражение в размерах входов сетей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def classifier_mlp(input_x):\n",
    "    # takes x to produce posterior class assignment probabilities\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                     input_var=input_x)\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),\n",
    "            name='cl_hid1')\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax,\n",
    "            name='cl_out')\n",
    "    return l_out\n",
    "\n",
    "def vae_encoder_cond(input_xy):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 28 * 28 + NUMBER_OF_DIGITS),\n",
    "                                     input_var=input_xy)\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "        l_in, num_units=256,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform(),\n",
    "        name='e_hid')\n",
    "    l_out_loc = lasagne.layers.DenseLayer(\n",
    "        l_hid1, num_units=HIDDEN_DIM,\n",
    "        nonlinearity=None,\n",
    "        name='e_mean')\n",
    "    l_out_scale = lasagne.layers.DenseLayer(\n",
    "        l_hid1, num_units=HIDDEN_DIM,\n",
    "        nonlinearity=lasagne.nonlinearities.softplus,\n",
    "        name='e_scale')\n",
    "    \n",
    "    return l_out_loc, l_out_scale\n",
    "    \n",
    "    \n",
    "def vae_decoder_cond(input_zy):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, HIDDEN_DIM + NUMBER_OF_DIGITS),\n",
    "                                     input_var=input_zy)\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),\n",
    "            name='d_hid1')\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=28 * 28,\n",
    "            nonlinearity=None,\n",
    "            name='d_out')\n",
    "    l_out = lasagne.layers.ReshapeLayer(l_out, shape=(-1, 1, 28, 28))\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "При обучении мы будем вычислять выходы нейросети на всех возможных значениях $y$, все они нужны в 95% случаев для подсчета нижней оценки на обоснованность. Для этого здесь написаны две вспомонательные функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_x = T.tensor4('input_x')\n",
    "input_y = T.ivector('input_y')\n",
    "input_p = T.bvector('input_present')\n",
    "\n",
    "def add_all_possible_labels(input_x):\n",
    "    # создает десять копий объекта из батча и приписывает к каждой из них код для y\n",
    "    input_x = T.reshape(input_x, newshape=(BATCH_SIZE, -1))\n",
    "    input_x = T.repeat(input_x, repeats=NUMBER_OF_DIGITS, axis=0)\n",
    "    input_y = T.repeat(T.eye(NUMBER_OF_DIGITS), repeats=BATCH_SIZE, axis=0)\n",
    "    input_xy = T.concatenate([input_x, input_y], axis=1)\n",
    "    return input_xy\n",
    "\n",
    "def add_corresponding_labels(input_z):\n",
    "    # приписывает код n % 10 (остаток деления) для n объекта в батче\n",
    "    input_y = T.repeat(T.eye(NUMBER_OF_DIGITS), repeats=BATCH_SIZE, axis=0)\n",
    "    input_zy = T.concatenate([input_z, input_y], axis=1)\n",
    "    return input_zy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Модель вывода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_xy = add_all_possible_labels(input_x)\n",
    "encoder_mean, encoder_scale = vae_encoder_cond(input_xy)\n",
    "qz_xy = MultivariateNormalDiag(\n",
    "    lasagne.layers.get_output(encoder_mean), \n",
    "    lasagne.layers.get_output(encoder_scale)\n",
    ")\n",
    "\n",
    "input_zy = add_corresponding_labels(qz_xy.sample())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Генеративная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decoder_logits = vae_decoder_cond(input_zy)\n",
    "pz = MultivariateNormalDiag(T.zeros((NUMBER_OF_DIGITS * BATCH_SIZE, HIDDEN_DIM)),\n",
    "                            T.ones((NUMBER_OF_DIGITS * BATCH_SIZE, HIDDEN_DIM)))\n",
    "# здесь мы не стали реализовывать отдельный класс\n",
    "p_y = -np.log(NUMBER_OF_DIGITS * np.ones([BATCH_SIZE * NUMBER_OF_DIGITS]))\n",
    "\n",
    "px_zy = BinaryVector(\n",
    "    lasagne.layers.get_output(decoder_logits)\n",
    ")\n",
    "\n",
    "classifier = classifier_mlp(input_x)\n",
    "qy_x_probs = lasagne.layers.get_output(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "alpha = 5.\n",
    "\n",
    "elbo_vec = (+ p_y\n",
    "            + px_zy.log_prob(T.repeat(input_x, repeats=NUMBER_OF_DIGITS, axis=0))\n",
    "            + pz.log_prob(qz_xy.sample())\n",
    "            - qz_xy.log_prob(qz_xy.sample()))\n",
    "elbo_vec = T.reshape(elbo_vec, newshape=(BATCH_SIZE, NUMBER_OF_DIGITS))\n",
    "\n",
    "elbo_vec = (\n",
    "    input_p * T.sum(elbo_vec * lasagne.utils.one_hot(input_y, m=NUMBER_OF_DIGITS), axis=1)\n",
    "    + (1 - input_p) * T.sum(qy_x_probs * elbo_vec - qy_x_probs * T.log(qy_x_probs), axis=1)\n",
    ")\n",
    "\n",
    "loss = T.mean(elbo_vec - alpha * input_p * lasagne.objectives.categorical_crossentropy(qy_x_probs, input_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(\n",
    "    [encoder_mean, encoder_scale, decoder_logits, classifier]\n",
    ")\n",
    "updates = lasagne.updates.adam(-loss, params)\n",
    "\n",
    "train_fn = theano.function([input_x, input_y, input_p], loss, updates=updates)\n",
    "accuracy = theano.function(\n",
    "    [input_x, input_y],\n",
    "    T.mean(T.eq(T.argmax(qy_x_probs, axis=1), input_y), dtype=theano.config.floatX)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, present=present, batchsize=BATCH_SIZE):\n",
    "        inputs, targets, batch_present = batch\n",
    "        inputs = np.random.rand(*inputs.shape) < inputs\n",
    "        train_err += train_fn(inputs, targets, batch_present)\n",
    "        train_batches += 1\n",
    "    \n",
    "    test_accuracy = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, batchsize=BATCH_SIZE, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        inputs = np.random.rand(*inputs.shape) < inputs\n",
    "        test_accuracy += accuracy(inputs, targets)\n",
    "        test_batches += 1\n",
    "    \n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "          epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"Train elbo {}\".format(train_err/train_batches))\n",
    "    print(\"Test accuracy {}\".format(test_accuracy/test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Задание*\n",
    "\n",
    "Ниже приведен код, генерирующий случайные цифры из заданного класса.\n",
    "\n",
    "Эксперементируя с архитектурами сети и параметрами модели, попробуйте обучить модель, для которой успешно выполняется это сэмплирование (см. эксперементальные результаты статьи https://arxiv.org/pdf/1406.5298.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_z = T.matrix('input_z')\n",
    "\n",
    "decode_a_code = theano.function(\n",
    "    [input_z],\n",
    "    lasagne.layers.get_output(decoder_logits, input_z),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "digit_to_draw = 4\n",
    "\n",
    "z_samples = np.random.randn(64, HIDDEN_DIM)\n",
    "y_samples = np.zeros((64, NUMBER_OF_DIGITS))\n",
    "y_samples[:, digit_to_draw] = 1\n",
    "zy_samples = np.concatenate([z_samples, y_samples], axis=1)\n",
    "\n",
    "decoded_images = decode_a_code(zy_samples)\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(8, 8),\n",
    "    subplot_kw={'xticks': [], 'yticks': []}\n",
    ")\n",
    "fig.subplots_adjust(hspace=0.04, wspace=0.02)\n",
    "\n",
    "for ax, i in zip(axes.flat, range(64)):\n",
    "    ax.imshow(decoded_images[i].reshape((28, 28)), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
